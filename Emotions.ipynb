{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from gpt2 import GPT2LanguageModel\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def get_next_words(model, context, words, depth):\n",
    "    # get next \"word\" given context\n",
    "    if depth == 0:\n",
    "        return words\n",
    "    new_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        logits = model.predict(context, word)\n",
    "        # take the one with the highest probability\n",
    "        # next_word_logit, next_index = logits.topk(1)\n",
    "        next_index = torch.argmax(logits)\n",
    "        next_word = model[next_index.item()]\n",
    "\n",
    "        new_words.append(word + next_word)\n",
    "    return get_next_words(model, context, new_words, depth-1)\n",
    "\n",
    "def get_probabilities_words(model, context, words):\n",
    "    # encode words to tokens\n",
    "    # Add a whitespace to the comparisons if there is no trailing whitespace in context\n",
    "    encoded_comp = model.tokenizer.encode(\" \" + words if context[-1] != \" \" else words)\n",
    "    # If comparison is composed of multiple words find them one after the other\n",
    "    probs = []\n",
    "    new_context = context\n",
    "    for token in encoded_comp:\n",
    "        logits = model.predict(new_context, None)\n",
    "        probabilities = softmax(logits, dim=-1)\n",
    "        probs.append(probabilities[token].item())\n",
    "        # feed in model with new context (oldContext+token) and get probability\n",
    "        new_context += model.tokenizer.decode([token])\n",
    "#     print(encoded_comp, model.tokenizer.decode(encoded_comp), probs)\n",
    "    # TODO calculate with proper bayesian probability?\n",
    "    return probs, words\n",
    "\n",
    "model_117M = GPT2LanguageModel(model_name='117M')\n",
    "model_345M = GPT2LanguageModel(model_name='345M')\n",
    "\n",
    "model_name = \"345M\"\n",
    "model = model_117M if model_name == \"117M\" else model_345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE A trailing whitespace gives other output than without\n",
    "context = \"Global warming is a\"\n",
    "comparisons = [\"big myth\", \"myth\", \"fascinating\", \"hoax\", \"farce\", \"onomatopeia\"]\n",
    "\n",
    "with open(\"emotions.txt\", \"r\") as f:\n",
    "    emotions = f.readlines()\n",
    "    \n",
    "emotions = [e.strip() for e in emotions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context =  Global warming is a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 269/269 [00:53<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 out of  269\n"
     ]
    }
   ],
   "source": [
    "# sorted_probabilities, order = torch.sort(probabilities, descending=True)\n",
    "# words = [model[idx.item()].strip() for idx in order]\n",
    "# print(words)\n",
    "\n",
    "# filter words given comparison list\n",
    "print(\"Context = \", context)\n",
    "probsWithWords = []\n",
    "for comp in tqdm.tqdm(emotions):\n",
    "    probsWithWords.append(get_probabilities_words(model, context, comp))\n",
    "    # print(f'With probability of {probs[-1]}: \"{comp}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 out of  269\n"
     ]
    }
   ],
   "source": [
    "singleWordTokens = []\n",
    "otherWordTokens = []\n",
    "for entry in probsWithWords:\n",
    "    if len(entry[0]) <= 2:\n",
    "        singleWordTokens.append(entry[1])\n",
    "    else:\n",
    "        otherWordTokens.append(entry[1])\n",
    "#         print(entry[1], len(entry[0]))\n",
    "print(len(singleWordTokens), \"out of \", len(probsWithWords))\n",
    "with open(\"SingleTokenEmotions.txt\", \"w\") as f:\n",
    "    for word in singleWordTokens:\n",
    "        f.write(word+\"\\n\")\n",
    "with open(\"MultipleTokenEmotions.txt\", \"w\") as f:\n",
    "    for word in otherWordTokens:\n",
    "        f.write(word+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10\n",
    "\n",
    "logits = model.predict(context, None)\n",
    "\n",
    "probabilities = softmax(logits, dim=-1)\n",
    "best_logits, best_indices = logits.topk(topk)\n",
    "best_words = [model[idx.item()] for idx in best_indices]\n",
    "\n",
    "most_likely_words = get_next_words(model, context, best_words, 0)\n",
    "\n",
    "best_probabilities = probabilities[best_indices].tolist()\n",
    "\n",
    "print(\"Input: \", context)\n",
    "for i, prob in enumerate(best_probabilities):\n",
    "    print(f\"{prob*100:.3f}%: {most_likely_words[i].strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
